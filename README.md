# Mobiles Discount Prediction (Flipkart + Amazon)

> **End‑to‑end data product:** Web scraping → Data validation & cleaning → Unified e‑commerce dataset → EDA & feature engineering → Discount prediction model → Streamlit app

![status-badge](https://img.shields.io/badge/status-active-brightgreen) ![python](https://img.shields.io/badge/Python-3.10%2B-blue) ![license](https://img.shields.io/badge/License-MIT-lightgrey) ![build](https://img.shields.io/badge/CI-GitHub%20Actions-blueviolet)

---

## 📌 Project Overview

This repository contains a **production‑ready, reproducible pipeline** that collects smartphone listings from **Flipkart** and **Amazon** (target: **≥ 500 mobiles per platform**), performs rigorous **data cleaning & validation**, merges both sources into a single **`ecommerce_final.csv`** dataset, and trains a model to **predict discount price / deal price**. A minimal **Streamlit app** is included for interactive exploration and inference.

**Core deliverables**

* Robust web scrapers for Flipkart & Amazon
* Automated validation (schema + data quality checks)
* Clean, tidy per‑platform datasets and a unified final dataset
* EDA notebooks + feature engineering
* ML pipeline to predict `Discount_Price`
* Streamlit app to demo predictions

---

> The **`notebooks/Mobile_Discount_Price_Prediction.ipynb`** notebook documents EDA, preprocessing decisions, and model experiments.

---

## 🧾 Data Dictionary

Each record represents a mobile phone listing. After cleaning, both sources conform to **one schema**:

| Column             | Type     | Description                                          |
| ------------------ | -------- | ---------------------------------------------------- |
| `brand`            | string   | Phone brand (normalized casing; e.g., "Samsung")     |
| `model`            | string   | Model name normalized (e.g., "Galaxy S23 8GB/128GB") |
| `ram_gb`           | float    | RAM in GB                                            |
| `rom_gb`           | float    | Storage in GB                                        |
| `display_size_in`  | float    | Screen size (inches)                                 |
| `battery_mah`      | int      | Battery capacity (mAh)                               |
| `rear_cam_mp`      | float    | Rear camera megapixels (max / primary)               |
| `front_cam_mp`     | float    | Front camera megapixels                              |
| `processor`        | string   | Chipset/processor family                             |
| `rating`           | float    | Average user rating                                  |
| `rating_count`     | int      | Number of ratings                                    |
| `review_count`     | int      | Number of reviews                                    |
| `mrp`              | float    | Listed MRP (₹)                                       |
| `price`            | float    | Current listed price (₹)                             |
| `discount_percent` | float    | `(mrp - price)/mrp * 100`                            |
| `platform`         | category | `flipkart` or `amazon`                               |
| `url`              | string   | Product detail URL                                   |
| `scrape_ts`        | datetime | Scrape timestamp (UTC)                               |
| `discount_price` | float    | Final Price                            |

> **Target(s)** for modeling: `discount_price` (primary).

---

## 🕸️ Data Collection (Web Scraping)

* **Volume:** Aim for **≥ 500 unique mobiles** per platform (Flipkart & Amazon).
* **Method:** Requests + parsing (or Selenium for dynamic sections); polite crawling with user‑agent rotation and throttling.
* **Keys captured:** Title, brand, model, specs (RAM/ROM, display, battery, cameras, processor), price components, ratings, counts, URL.
* **De‑duplication:** Normalize (brand, model, RAM, ROM) and drop near‑duplicates by URL & title fingerprints.

**Run scrapers**

```bash
# Flipkart
python -m src.scrapers.flipkart_scraper \
  --pages 50 --min_items 500 --out data/raw/flipkart_mobiles_raw.jsonl

# Amazon
python -m src.scrapers.amazon_scraper \
  --pages 50 --min_items 500 --out data/raw/amazon_mobiles_raw.jsonl
```

> Configure secrets (proxies, keys if any) via **`.env`** (copy from `.env.example`). Respect site terms and robots; for research/learning only.

---

## 🧹 Cleaning & ✅ Validation

Cleaning and validation bring both sources to the **unified schema** above.

**Major steps**

1. **Type Coercion:** Parse numerics from strings (e.g., `"8 GB" → 8`).
2. **Unit Normalization:** Inches, mAh, MP; strip symbols; convert ₹.
3. **Brand/Model Canonicalization:** Regex + rules to unify brand/model tokens.
4. **Price Logic:** Ensure `mrp ≥ price`; recompute `discount_percent`.
5. **Outlier & Anomaly Checks:** Winsorize extreme prices/specs; flag negatives/zeros.
6. **Deduplication:** Title fingerprinting + key tuple `(brand, model, ram_gb, rom_gb)`.
7. **Schema Validation:** `pandera`/`pydantic`; optional `Great Expectations` suite.
8. **Missingness Handling:** Impute or drop based on thresholds; document decisions.
9. **Handleing Unwanted data** Using IsolationForest checking the outlaiers
**Run cleaning & merge**

```bash
# Validate + clean per-platform
python -m src.data.clean_merge \
  --flipkart data/raw/flipkart_mobiles_raw.jsonl \
  --amazon  data/raw/amazon_mobiles_raw.jsonl \
  --out_dir data/processed

# Outputs
#  - data/processed/flipkart_mobiles_clean.csv
#  - data/processed/amazon_mobiles_clean.csv
#  - data/processed/ecommerce_final.csv
```

**Automated checks** (examples)

* No nulls in `brand`, `model`, `price`, `platform`, `url`
* `0 < discount_percent < 95`
* `3 ≤ display_size_in ≤ 8`, `1000 ≤ battery_mah ≤ 10000`
* `rating ∈ [0, 5]`, counts ≥ 0

---

## 🧪 EDA & Feature Engineering

Use the notebook to explore distributions and relationships:

* **Univariate:** Histograms/boxplots (price, discount, ratings)
* **Bivariate:** Price vs specs, discount vs platform/brand
* **Multivariate:** Correlation heatmap; feature importance

**Feature ideas**

* `price_per_gb = price / (ram_gb + rom_gb/4)` (proxy value metric)
* `camera_score = 0.7*rear_cam_mp + 0.3*front_cam_mp`
* `brand_tier` (A/B/C based on median price)
* `popularity = log1p(rating_count)`
* One‑hot encode `platform`, Target Encodeing `brand`, `processor`

---

## 🤖 Modeling

Goal: **predict `discount_price`** given specs & context.

**Pipeline**

* Train/val/test split with time‑aware holdout by `scrape_ts`
* Preprocess: imputation, scaling (numeric), encoding (categorical)
* Models: LinearReg, RandomForest, XGBoost/LightGBM; compare via cross‑val
* Metrics: **MAE**, **RMSE**, **R²**; calibration plots
* Persist best model as `artifacts/model.pkl` + `artifacts/preprocess.pkl`

 ## 🧠 Algorithms Used in Discount / Price Modeling

  | #  | Algorithm                               | Type                    | Purpose                                                                             |
| -- | --------------------------------------- | ----------------------- | ----------------------------------------------------------------------------------- |
| 1  | **Linear Regression**                   | Supervised (Regression) | Baseline model to predict continuous target like Price or Discount %                |
| 2  | **Ridge Regression**                    | Regularized Regression  | Handles multicollinearity and prevents overfitting                                  |
| 3  | **Lasso Regression**                    | Regularized Regression  | Performs feature selection by shrinking less important coefficients to zero         |
| 4  | **Decision Tree Regressor**             | Non-linear Regression   | Captures non-linear relationships between price and product features                |
| 5  | **Random Forest Regressor**             | Ensemble (Bagging)      | Improves accuracy by combining multiple decision trees                              |
| 6  | **Gradient Boosting Regressor**         | Ensemble (Boosting)     | Sequentially improves weak learners for better performance                          |
| 7  | **XGBoost Regressor**                   | Advanced Boosting       | Highly efficient boosting algorithm, often gives top performance on structured data |
| 8  | **CatBoost Regressor**                  | Boosting                | Handles categorical variables automatically, very effective for product data        |
| 9  | **LightGBM Regressor**                  | Boosting                | Fast and memory-efficient gradient boosting, good for large datasets                |
| 10 | **K-Nearest Neighbors (KNN) Regressor** | Instance-based          | Predicts based on similar historical data points                                    |
| 11 | **Support Vector Regression (SVR)**     | Kernel-based            | Works well on small- to medium-sized datasets with complex relationships            |
| 12 | **Neural Network / MLP Regressor**      | Deep Learning           | Learns complex non-linear relationships for price prediction                        |


**Train & evaluate**

```bash
python -m src.models.train \
  --data data/processed/ecommerce_final.csv \
  --target discount_percent \
  --out_dir artifacts

python -m src.models.evaluate \
  --data data/processed/ecommerce_final.csv \
  --model artifacts/model.pkl
```

---

## 🖥️ Streamlit App

Quick interactive UI to inspect the dataset and run predictions.

```bash
streamlit run src/app/app.py
```

Features:

* Filters by brand/specs
* Visuals (discount_price, discount distributions)
* On‑the‑fly prediction panel

