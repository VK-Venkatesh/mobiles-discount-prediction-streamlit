# Mobiles Discount Prediction (Flipkart + Amazon)

> **Endâ€‘toâ€‘end data product:** Web scraping â†’ Data validation & cleaning â†’ Unified eâ€‘commerce dataset â†’ EDA & feature engineering â†’ Discount prediction model â†’ Streamlit app

![status-badge](https://img.shields.io/badge/status-active-brightgreen) ![python](https://img.shields.io/badge/Python-3.10%2B-blue) ![license](https://img.shields.io/badge/License-MIT-lightgrey) ![build](https://img.shields.io/badge/CI-GitHub%20Actions-blueviolet)

---

## ðŸ“Œ Project Overview

This repository contains a **productionâ€‘ready, reproducible pipeline** that collects smartphone listings from **Flipkart** and **Amazon** (target: **â‰¥ 500 mobiles per platform**), performs rigorous **data cleaning & validation**, merges both sources into a single **`ecommerce_final.csv`** dataset, and trains a model to **predict discount percentage / deal price**. A minimal **Streamlit app** is included for interactive exploration and inference.

**Core deliverables**

* Robust web scrapers for Flipkart & Amazon
* Automated validation (schema + data quality checks)
* Clean, tidy perâ€‘platform datasets and a unified final dataset
* EDA notebooks + feature engineering
* ML pipeline to predict `Discount_%` (and optionally final price)
* Streamlit app to demo predictions

---

> The **`notebooks/Mobile_Discount_Price_Prediction.ipynb`** notebook documents EDA, preprocessing decisions, and model experiments.

---

## ðŸ§¾ Data Dictionary (Unified Schema)

Each record represents a mobile phone listing. After cleaning, both sources conform to **one schema**:

| Column             | Type     | Description                                          |
| ------------------ | -------- | ---------------------------------------------------- |
| `brand`            | string   | Phone brand (normalized casing; e.g., "Samsung")     |
| `model`            | string   | Model name normalized (e.g., "Galaxy S23 8GB/128GB") |
| `ram_gb`           | float    | RAM in GB                                            |
| `rom_gb`           | float    | Storage in GB                                        |
| `display_size_in`  | float    | Screen size (inches)                                 |
| `battery_mah`      | int      | Battery capacity (mAh)                               |
| `rear_cam_mp`      | float    | Rear camera megapixels (max / primary)               |
| `front_cam_mp`     | float    | Front camera megapixels                              |
| `processor`        | string   | Chipset/processor family                             |
| `rating`           | float    | Average user rating                                  |
| `rating_count`     | int      | Number of ratings                                    |
| `review_count`     | int      | Number of reviews                                    |
| `mrp`              | float    | Listed MRP (â‚¹)                                       |
| `price`            | float    | Current listed price (â‚¹)                             |
| `discount_percent` | float    | `(mrp - price)/mrp * 100`                            |
| `platform`         | category | `flipkart` or `amazon`                               |
| `url`              | string   | Product detail URL                                   |
| `scrape_ts`        | datetime | Scrape timestamp (UTC)                               |

> **Target(s)** for modeling: `discount_percent` (primary), and optionally `price`.

---

## ðŸ•¸ï¸ Data Collection (Web Scraping)

* **Volume:** Aim for **â‰¥ 500 unique mobiles** per platform (Flipkart & Amazon).
* **Method:** Requests + parsing (or Selenium for dynamic sections); polite crawling with userâ€‘agent rotation and throttling.
* **Keys captured:** Title, brand, model, specs (RAM/ROM, display, battery, cameras, processor), price components, ratings, counts, URL.
* **Deâ€‘duplication:** Normalize (brand, model, RAM, ROM) and drop nearâ€‘duplicates by URL & title fingerprints.

**Run scrapers**

```bash
# Flipkart
python -m src.scrapers.flipkart_scraper \
  --pages 50 --min_items 500 --out data/raw/flipkart_mobiles_raw.jsonl

# Amazon
python -m src.scrapers.amazon_scraper \
  --pages 50 --min_items 500 --out data/raw/amazon_mobiles_raw.jsonl
```

> Configure secrets (proxies, keys if any) via **`.env`** (copy from `.env.example`). Respect site terms and robots; for research/learning only.

---

## ðŸ§¹ Cleaning & âœ… Validation

Cleaning and validation bring both sources to the **unified schema** above.

**Major steps**

1. **Type Coercion:** Parse numerics from strings (e.g., `"8 GB" â†’ 8`).
2. **Unit Normalization:** Inches, mAh, MP; strip symbols; convert â‚¹.
3. **Brand/Model Canonicalization:** Regex + rules to unify brand/model tokens.
4. **Price Logic:** Ensure `mrp â‰¥ price`; recompute `discount_percent`.
5. **Outlier & Anomaly Checks:** Winsorize extreme prices/specs; flag negatives/zeros.
6. **Deduplication:** Title fingerprinting + key tuple `(brand, model, ram_gb, rom_gb)`.
7. **Schema Validation:** `pandera`/`pydantic`; optional `Great Expectations` suite.
8. **Missingness Handling:** Impute or drop based on thresholds; document decisions.

**Run cleaning & merge**

```bash
# Validate + clean per-platform
python -m src.data.clean_merge \
  --flipkart data/raw/flipkart_mobiles_raw.jsonl \
  --amazon  data/raw/amazon_mobiles_raw.jsonl \
  --out_dir data/processed

# Outputs
#  - data/processed/flipkart_mobiles_clean.csv
#  - data/processed/amazon_mobiles_clean.csv
#  - data/processed/ecommerce_final.csv
```

**Automated checks** (examples)

* No nulls in `brand`, `model`, `price`, `platform`, `url`
* `0 < discount_percent < 95`
* `3 â‰¤ display_size_in â‰¤ 8`, `1000 â‰¤ battery_mah â‰¤ 10000`
* `rating âˆˆ [0, 5]`, counts â‰¥ 0

---

## ðŸ§ª EDA & Feature Engineering

Use the notebook to explore distributions and relationships:

* **Univariate:** Histograms/boxplots (price, discount, ratings)
* **Bivariate:** Price vs specs, discount vs platform/brand
* **Multivariate:** Correlation heatmap; feature importance

**Feature ideas**

* `price_per_gb = price / (ram_gb + rom_gb/4)` (proxy value metric)
* `camera_score = 0.7*rear_cam_mp + 0.3*front_cam_mp`
* `brand_tier` (A/B/C based on median price)
* `popularity = log1p(rating_count)`
* Oneâ€‘hot encode `platform`, topâ€‘K `brand`, hash `processor`

---

## ðŸ¤– Modeling

Goal: **predict `discount_percent`** given specs & context.

**Pipeline**

* Train/val/test split with timeâ€‘aware holdout by `scrape_ts`
* Preprocess: imputation, scaling (numeric), encoding (categorical)
* Models: LinearReg, RandomForest, XGBoost/LightGBM; compare via crossâ€‘val
* Metrics: **MAE**, **RMSE**, **RÂ²**; calibration plots
* Persist best model as `artifacts/model.pkl` + `artifacts/preprocess.pkl`

**Train & evaluate**

```bash
python -m src.models.train \
  --data data/processed/ecommerce_final.csv \
  --target discount_percent \
  --out_dir artifacts

python -m src.models.evaluate \
  --data data/processed/ecommerce_final.csv \
  --model artifacts/model.pkl
```

---

## ðŸ–¥ï¸ Streamlit App

Quick interactive UI to inspect the dataset and run predictions.

```bash
streamlit run src/app/app.py
```

Features:

* Filters by brand/specs
* Visuals (price vs specs, discount distributions)
* Onâ€‘theâ€‘fly prediction panel

