# Mobiles Discount Prediction (Flipkart + Amazon)

> **End‚Äëto‚Äëend data product:** Web scraping ‚Üí Data validation & cleaning ‚Üí Unified e‚Äëcommerce dataset ‚Üí EDA & feature engineering ‚Üí Discount prediction model ‚Üí Streamlit app

![status-badge](https://img.shields.io/badge/status-active-brightgreen) ![python](https://img.shields.io/badge/Python-3.10%2B-blue) ![license](https://img.shields.io/badge/License-MIT-lightgrey) ![build](https://img.shields.io/badge/CI-GitHub%20Actions-blueviolet)

---

## üìå Project Overview

This repository contains a **production‚Äëready, reproducible pipeline** that collects smartphone listings from **Flipkart** and **Amazon** (target: **‚â• 500 mobiles per platform**), performs rigorous **data cleaning & validation**, merges both sources into a single **`ecommerce_final.csv`** dataset, and trains a model to **predict discount percentage / deal price**. A minimal **Streamlit app** is included for interactive exploration and inference.

**Core deliverables**

* Robust web scrapers for Flipkart & Amazon
* Automated validation (schema + data quality checks)
* Clean, tidy per‚Äëplatform datasets and a unified final dataset
* EDA notebooks + feature engineering
* ML pipeline to predict `Discount_%` (and optionally final price)
* Streamlit app to demo predictions

---

## üóÇÔ∏è Repository Structure

```
Mobiles_Discount-Prediction-Streamlit/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flipkart_mobiles_raw.jsonl
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ amazon_mobiles_raw.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ interim/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flipkart_mobiles_interim.parquet
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ amazon_mobiles_interim.parquet
‚îÇ   ‚îî‚îÄ‚îÄ processed/
‚îÇ       ‚îú‚îÄ‚îÄ flipkart_mobiles_clean.csv
‚îÇ       ‚îú‚îÄ‚îÄ amazon_mobiles_clean.csv
‚îÇ       ‚îî‚îÄ‚îÄ ecommerce_final.csv
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ Mobile_Discount_Price_Prediction.ipynb
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ scrapers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flipkart_scraper.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ amazon_scraper.py
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clean_merge.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validate_schema.py
‚îÇ   ‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ build_features.py
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluate.py
‚îÇ   ‚îî‚îÄ‚îÄ app/
‚îÇ       ‚îî‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_schema.py
‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ environment.yml
‚îú‚îÄ‚îÄ Makefile
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ LICENSE
‚îî‚îÄ‚îÄ README.md
```

> The **`notebooks/Mobile_Discount_Price_Prediction.ipynb`** notebook documents EDA, preprocessing decisions, and model experiments.

---

## üßæ Data Dictionary (Unified Schema)

Each record represents a mobile phone listing. After cleaning, both sources conform to **one schema**:

| Column             | Type     | Description                                          |
| ------------------ | -------- | ---------------------------------------------------- |
| `brand`            | string   | Phone brand (normalized casing; e.g., "Samsung")     |
| `model`            | string   | Model name normalized (e.g., "Galaxy S23 8GB/128GB") |
| `ram_gb`           | float    | RAM in GB                                            |
| `rom_gb`           | float    | Storage in GB                                        |
| `display_size_in`  | float    | Screen size (inches)                                 |
| `battery_mah`      | int      | Battery capacity (mAh)                               |
| `rear_cam_mp`      | float    | Rear camera megapixels (max / primary)               |
| `front_cam_mp`     | float    | Front camera megapixels                              |
| `processor`        | string   | Chipset/processor family                             |
| `rating`           | float    | Average user rating                                  |
| `rating_count`     | int      | Number of ratings                                    |
| `review_count`     | int      | Number of reviews                                    |
| `mrp`              | float    | Listed MRP (‚Çπ)                                       |
| `price`            | float    | Current listed price (‚Çπ)                             |
| `discount_percent` | float    | `(mrp - price)/mrp * 100`                            |
| `platform`         | category | `flipkart` or `amazon`                               |
| `url`              | string   | Product detail URL                                   |
| `scrape_ts`        | datetime | Scrape timestamp (UTC)                               |

> **Target(s)** for modeling: `discount_percent` (primary), and optionally `price`.

---

## üï∏Ô∏è Data Collection (Web Scraping)

* **Volume:** Aim for **‚â• 500 unique mobiles** per platform (Flipkart & Amazon).
* **Method:** Requests + parsing (or Selenium for dynamic sections); polite crawling with user‚Äëagent rotation and throttling.
* **Keys captured:** Title, brand, model, specs (RAM/ROM, display, battery, cameras, processor), price components, ratings, counts, URL.
* **De‚Äëduplication:** Normalize (brand, model, RAM, ROM) and drop near‚Äëduplicates by URL & title fingerprints.

**Run scrapers**

```bash
# Flipkart
python -m src.scrapers.flipkart_scraper \
  --pages 50 --min_items 500 --out data/raw/flipkart_mobiles_raw.jsonl

# Amazon
python -m src.scrapers.amazon_scraper \
  --pages 50 --min_items 500 --out data/raw/amazon_mobiles_raw.jsonl
```

> Configure secrets (proxies, keys if any) via **`.env`** (copy from `.env.example`). Respect site terms and robots; for research/learning only.

---

## üßπ Cleaning & ‚úÖ Validation

Cleaning and validation bring both sources to the **unified schema** above.

**Major steps**

1. **Type Coercion:** Parse numerics from strings (e.g., `"8 GB" ‚Üí 8`).
2. **Unit Normalization:** Inches, mAh, MP; strip symbols; convert ‚Çπ.
3. **Brand/Model Canonicalization:** Regex + rules to unify brand/model tokens.
4. **Price Logic:** Ensure `mrp ‚â• price`; recompute `discount_percent`.
5. **Outlier & Anomaly Checks:** Winsorize extreme prices/specs; flag negatives/zeros.
6. **Deduplication:** Title fingerprinting + key tuple `(brand, model, ram_gb, rom_gb)`.
7. **Schema Validation:** `pandera`/`pydantic`; optional `Great Expectations` suite.
8. **Missingness Handling:** Impute or drop based on thresholds; document decisions.

**Run cleaning & merge**

```bash
# Validate + clean per-platform
python -m src.data.clean_merge \
  --flipkart data/raw/flipkart_mobiles_raw.jsonl \
  --amazon  data/raw/amazon_mobiles_raw.jsonl \
  --out_dir data/processed

# Outputs
#  - data/processed/flipkart_mobiles_clean.csv
#  - data/processed/amazon_mobiles_clean.csv
#  - data/processed/ecommerce_final.csv
```

**Automated checks** (examples)

* No nulls in `brand`, `model`, `price`, `platform`, `url`
* `0 < discount_percent < 95`
* `3 ‚â§ display_size_in ‚â§ 8`, `1000 ‚â§ battery_mah ‚â§ 10000`
* `rating ‚àà [0, 5]`, counts ‚â• 0

---

## üß™ EDA & Feature Engineering

Use the notebook to explore distributions and relationships:

* **Univariate:** Histograms/boxplots (price, discount, ratings)
* **Bivariate:** Price vs specs, discount vs platform/brand
* **Multivariate:** Correlation heatmap; feature importance

**Feature ideas**

* `price_per_gb = price / (ram_gb + rom_gb/4)` (proxy value metric)
* `camera_score = 0.7*rear_cam_mp + 0.3*front_cam_mp`
* `brand_tier` (A/B/C based on median price)
* `popularity = log1p(rating_count)`
* One‚Äëhot encode `platform`, top‚ÄëK `brand`, hash `processor`

---

## ü§ñ Modeling

Goal: **predict `discount_percent`** given specs & context.

**Pipeline**

* Train/val/test split with time‚Äëaware holdout by `scrape_ts`
* Preprocess: imputation, scaling (numeric), encoding (categorical)
* Models: LinearReg, RandomForest, XGBoost/LightGBM; compare via cross‚Äëval
* Metrics: **MAE**, **RMSE**, **R¬≤**; calibration plots
* Persist best model as `artifacts/model.pkl` + `artifacts/preprocess.pkl`

**Train & evaluate**

```bash
python -m src.models.train \
  --data data/processed/ecommerce_final.csv \
  --target discount_percent \
  --out_dir artifacts

python -m src.models.evaluate \
  --data data/processed/ecommerce_final.csv \
  --model artifacts/model.pkl
```

---

## üñ•Ô∏è Streamlit App

Quick interactive UI to inspect the dataset and run predictions.

```bash
streamlit run src/app/app.py
```

Features:

* Filters by brand/specs
* Visuals (price vs specs, discount distributions)
* On‚Äëthe‚Äëfly prediction panel

---

## üöÄ Getting Started

**Option A ‚Äì venv + pip**

```bash
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt
```

**Option B ‚Äì Conda**

```bash\ nconda env create -f environment.yml
conda activate mobiles-discount
```

**Environment**

* Copy `.env.example` ‚Üí `.env` and set variables if needed (proxies, app secrets)

**Make it easy**

```bash
make scrape     # runs both scrapers with sensible defaults
make process    # cleaning + merge + schema checks
make train      # train best model
make app        # launch streamlit
```

---

## ‚úÖ Reproducibility & Quality

* **Fixed random seeds** for splits & models
* **Versioned artifacts** in `/artifacts`
* **Unit tests** for schema/utilities (`pytest`)
* **Pre‚Äëcommit hooks**: black, isort, flake8
* **CI**: GitHub Actions workflow for tests & lint

---

## üìà Results (example placeholders)

* `MAE` ‚âà *x.xx* %, `RMSE` ‚âà *y.yy* %, `R¬≤` ‚âà *0.zz*
* Feature importance: specs > popularity > brand\_tier > platform

> Replace with your actual metrics from `evaluate.py`.

---

## üîê Ethics & Compliance

* Respect site ToS and robots; throttle and randomize requests.
* For research/educational use. Do **not** overload source sites.
* Remove PII and comply with platform policies.

---

## üß≠ Roadmap

* [ ] Add scheduling for periodic scraping
* [ ] Great Expectations data docs
* [ ] Hyperparameter search with Optuna
* [ ] Model monitoring (drift + data quality)
* [ ] Dockerfile + deploy on Streamlit Cloud/Render

---

## ü§ù Contributing

PRs welcome! Please open an issue to discuss major changes.

---

## üßæ License

Released under the MIT License. See `LICENSE` for details.

---

## üôè Acknowledgements

Thanks to Flipkart and Amazon for publicly visible catalog pages used strictly for learning and research purposes.

---

## üîó Quick Commands (Copy‚ÄëPaste)

```bash
# 1) Scrape
python -m src.scrapers.flipkart_scraper --pages 50 --min_items 500 --out data/raw/flipkart_mobiles_raw.jsonl
python -m src.scrapers.amazon_scraper  --pages 50 --min_items 500 --out data/raw/amazon_mobiles_raw.jsonl

# 2) Clean + merge
python -m src.data.clean_merge --flipkart data/raw/flipkart_mobiles_raw.jsonl --amazon data/raw/amazon_mobiles_raw.jsonl --out_dir data/processed

# 3) Train + evaluate
python -m src.models.train --data data/processed/ecommerce_final.csv --target discount_percent --out_dir artifacts
python -m src.models.evaluate --data data/processed/ecommerce_final.csv --model artifacts/model.pkl

# 4) App
streamlit run src/app/app.py
```
